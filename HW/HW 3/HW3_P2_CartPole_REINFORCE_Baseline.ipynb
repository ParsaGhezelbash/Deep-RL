{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aii0rI8N_vbr"
      },
      "source": [
        "# Run this Notebook\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/DeepRLCourse/Homework-3-Questions/blob/main/HW3_P2_CartPole_REINFORCE_Baseline.ipynb)  \n",
        "[![Open in Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/DeepRLCourse/Homework-3-Questions/blob/main/HW3_P2_CartPole_REINFORCE_Baseline.ipynb)\n",
        "\n",
        "# HW3: REINFORCE with and without baseline in CartPole\n",
        "> - Full Name: **[Full Name]**\n",
        "> - Student ID: **[Stundet ID]**\n",
        "\n",
        "\n",
        "This notebook implements the **REINFORCE policy gradient algorithm** to train an agent in the **CartPole-v1** environment. It also compares the performance of **REINFORCE with and without a baseline** to assess its impact on training stability and efficiency.  \n",
        "\n",
        "**Grading Breakdown:**\n",
        "\n",
        "- Practical Implementation: 70 points\n",
        "- Conceptual Understanding: 30 points"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IuUkFbf1LKuf"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xYFVWXvOLKuf"
      },
      "source": [
        "All required packages are pre-installed if using Google Colab.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7sAsPKp6LKuf"
      },
      "source": [
        "Import the following libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "abSddpLXQz_c",
        "outputId": "df07fad5-a136-403e-a9ea-ec288a5c9712"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ],
      "source": [
        "# Imports\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Categorical\n",
        "import gym\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import base64\n",
        "import imageio\n",
        "import IPython\n",
        "import logging\n",
        "import warnings\n",
        "\n",
        "# Disable warnings\n",
        "logging.getLogger().setLevel(logging.ERROR)\n",
        "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
        "\n",
        "# DEVICE\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "DEVICE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZOBmWUkLKug"
      },
      "source": [
        "Configure Matplotlib for Interactive and XKCD-Style Plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "sDI1sUGpxZD9"
      },
      "outputs": [],
      "source": [
        "# Set up matplotlib\n",
        "is_ipython = 'inline' in matplotlib.get_backend()\n",
        "if is_ipython:\n",
        "    from IPython import display\n",
        "plt.ion()\n",
        "plt.xkcd(scale=1, length=100, randomness=2)\n",
        "matplotlib.rcParams['figure.figsize'] = (12, 6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8NpYHWWyLKug"
      },
      "source": [
        "Record and Embed Simulation Videos in Jupyter Notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJ1I-m_aLKug"
      },
      "source": [
        "<b>embed_mp4:</b> Converts an MP4 video into a base64-encoded HTML tag for display in Jupyter Notebook.\n",
        "<br>\n",
        "<b>record_simulation:</b> Runs a policy in the environment, records the simulation, and saves it as an MP4 video."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "edjZJb0epJ3r"
      },
      "outputs": [],
      "source": [
        "def embed_mp4(filename):\n",
        "    video = open(filename,'rb').read()\n",
        "    b64 = base64.b64encode(video)\n",
        "    tag = '''\n",
        "    <video width=\"640\" height=\"480\" controls>\n",
        "    <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\">\n",
        "    Your browser does not support the video tag.\n",
        "    </video>'''.format(b64.decode())\n",
        "    return IPython.display.HTML(tag)\n",
        "\n",
        "def record_simulation(env, policy_net, filename, episodes=1, fps=30):\n",
        "    filename = filename + \".mp4\"\n",
        "    with imageio.get_writer(filename, fps=fps) as video:\n",
        "        for _ in range(episodes):\n",
        "            state = env.reset()\n",
        "            frame = env.render()  # Capture the first frame\n",
        "            video.append_data(frame[0])\n",
        "\n",
        "            done = False\n",
        "            while not done:\n",
        "                state_tensor = torch.FloatTensor(state).to(DEVICE)\n",
        "                action = torch.argmax(policy_net(state_tensor)).item()\n",
        "                state, _, terminated, truncated, _ = env.step(action)\n",
        "\n",
        "                frame = env.render()  # Capture the frame after taking the action\n",
        "                video.append_data(frame[0])\n",
        "\n",
        "                done = terminated or truncated\n",
        "    return embed_mp4(filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ayNpYc4gLKuh"
      },
      "source": [
        "# Explore the environment  (5 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wiunu7QILKuh"
      },
      "source": [
        "Initialize CartPole Environment and Display State & Action Spaces"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3AELpRxkpJ3s",
        "outputId": "b52f54a0-a6a6-4408-c07f-e0dc5683b3e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Observation Space: Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)\n",
            "Action Space: Discrete(2)\n"
          ]
        }
      ],
      "source": [
        "# Create the CartPole environment\n",
        "env = gym.make(\"CartPole-v1\")\n",
        "\n",
        "# Print observation and action space\n",
        "print(\"Observation Space:\", env.observation_space)\n",
        "print(\"Action Space:\", env.action_space)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_8R1hqr_vbv"
      },
      "source": [
        "**Question 1: (5 points)**\n",
        "\n",
        "How are the observation and action spaces defined in the CartPole environment?\n",
        "\n",
        ".....</br>\n",
        ".....</br>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TYE9i_7jLKuh"
      },
      "source": [
        "Define a Random Policy for Action Selection  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "1QGgUwWALKuh"
      },
      "outputs": [],
      "source": [
        "class RandomPolicy(object):\n",
        "    def __init__(self, action_space_n):\n",
        "        self.action_space_n = action_space_n\n",
        "\n",
        "    def __call__(self, state):\n",
        "        action_values = torch.rand(self.action_space_n)\n",
        "        return action_values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTGEbmViLKuh"
      },
      "source": [
        "Visualize the random policy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 501
        },
        "id": "1aw82RC9LKuh",
        "outputId": "3ea4e358-294b-4fda-8654-1c1b0d1a9f0b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <video width=\"640\" height=\"480\" controls>\n",
              "    <source src=\"data:video/mp4;base64,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\" type=\"video/mp4\">\n",
              "    Your browser does not support the video tag.\n",
              "    </video>"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ],
      "source": [
        "random_policy = RandomPolicy(env.action_space.n)\n",
        "\n",
        "record_simulation(gym.make(\"CartPole-v1\", render_mode='rgb_array', new_step_api=True), random_policy, \"Video_CartPole_random_policy\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vzb6x_j0LKui"
      },
      "source": [
        "# Agent with REINFORCE: Baseline vs. No Baseline in CartPole-v1 (85 points)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5Mh1vakLKui"
      },
      "source": [
        "### Policy Network Definition (5 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AYRuIExMLKui"
      },
      "source": [
        "To define a neural network that represents the agent’s policy for selecting actions.\n",
        "\n",
        "The policy network takes the environment’s state as input and outputs a probability distribution over possible actions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "UFJORcIqpJ3s"
      },
      "outputs": [],
      "source": [
        "class PolicyNetwork(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(PolicyNetwork, self).__init__()\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc1 = nn.Linear(input_dim, 512)\n",
        "        self.fc2 = nn.Linear(512, output_dim)\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = self.fc2(self.relu(self.fc1(state)))\n",
        "        return torch.log_softmax(x, dim=-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ULn6LTQtLKui"
      },
      "source": [
        "### Computing Discounted Returns (10 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Icb1-UCfLKui"
      },
      "source": [
        "To implement a function that calculates the discounted return for each timestep in an episode.\n",
        "\n",
        "$$[\n",
        "G_t = \\sum_{k=0}^{T-t} \\gamma^k R_{t+k}\n",
        "]$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "JqYUeO4mpJ3t"
      },
      "outputs": [],
      "source": [
        "def compute_returns(rewards, gamma=0.99):\n",
        "    returns = []\n",
        "    G = 0\n",
        "    for r in reversed(rewards):\n",
        "        G = r + gamma * G\n",
        "        returns.append(G)\n",
        "    returns = list(reversed(returns))\n",
        "    return returns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPA4Z-K6_vbx"
      },
      "source": [
        "**Question 2: (5 points)**\n",
        "\n",
        "What is the role of the discount factor (𝛾) in reinforcement learning, and what happens when 𝛾=0 or 𝛾=1?\n",
        "\n",
        ".....</br>\n",
        ".....</br>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmmx02BNLKui"
      },
      "source": [
        "### Implementing the REINFORCE Algorithm (Without Baseline) (20 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1CUfNXaxLKui"
      },
      "source": [
        "To train the agent using the standard policy gradient method.\n",
        "The REINFORCE algorithm updates policy parameters by using the log-probability of actions multiplied by the discounted return.\n",
        "\n",
        "This algorithm optimizes a **stochastic policy** $( \\pi_{\\theta}(a_t \\mid s_t) )$ by updating its parameters in the direction that increases expected rewards. The update rule is based on the **policy gradient theorem**:  \n",
        "\n",
        "$$[\n",
        "\\theta \\leftarrow \\theta + \\alpha \\sum_{t=0}^{T} \\nabla_{\\theta} \\log \\pi_{\\theta}(a_t \\mid s_t) G_t\n",
        "]$$\n",
        "\n",
        "where:  \n",
        "\n",
        "- $( \\theta )$ are the policy parameters (weights of the neural network).  \n",
        "- $( \\alpha )$ is the learning rate.  \n",
        "- $( G_t )$ is the **discounted return** from timestep $( t )$:  \n",
        "\n",
        "- $( \\nabla_{\\theta} \\log \\pi_{\\theta}(a_t \\mid s_t) )$ is the gradient of the log-probability of the selected action, used to adjust the policy in the correct direction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "afoYGMXkLKuj"
      },
      "outputs": [],
      "source": [
        "def reinforce(env, policy_net, optimizer, num_episodes=1000, gamma=0.99):\n",
        "    rewards_per_episode = []\n",
        "    for episode in range(num_episodes):\n",
        "        state = env.reset()\n",
        "        trajectory, rewards = [], []\n",
        "\n",
        "        while True:\n",
        "            state_tensor = torch.tensor(state, dtype=torch.float32)\n",
        "            log_probs = policy_net(state_tensor).detach().numpy()\n",
        "            probs = np.exp(log_probs)\n",
        "\n",
        "            action = np.random.choice(len(probs), p=probs)\n",
        "            new_state, reward, done, _ = env.step(action)\n",
        "\n",
        "            trajectory.append((state, action, reward))\n",
        "            rewards.append(reward)\n",
        "\n",
        "            state = new_state\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        returns = compute_returns(rewards, gamma)\n",
        "        returns = torch.tensor(returns, dtype=torch.float32)\n",
        "\n",
        "        loss = 0\n",
        "        for (state, action, _), G in zip(trajectory, returns):\n",
        "            state_tensor = torch.tensor(state, dtype=torch.float32)\n",
        "            log_probs = policy_net(state_tensor)\n",
        "            loss -= log_probs[action] * G\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        rewards_per_episode.append(sum(rewards))\n",
        "\n",
        "        if (episode + 1) % 50 == 0:\n",
        "            print(f\"Episode {episode + 1}, Reward: {sum(rewards)}\")\n",
        "\n",
        "    return rewards_per_episode"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AuTozXroLKuj"
      },
      "source": [
        "### Value Network Definition (Baseline Network) (5 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7pV0dcokLKuj"
      },
      "source": [
        "Defines a neural network that approximates the value function, which estimates the expected return from a given state. The network takes the environment’s state as input and outputs a scalar value, representing the long-term reward the agent can expect from that state."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "QYDb1po7pJ3t"
      },
      "outputs": [],
      "source": [
        "class ValueNetwork(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(ValueNetwork, self).__init__()\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc1 = nn.Linear(input_dim, 512)\n",
        "        self.fc2 = nn.Linear(512, 1)\n",
        "\n",
        "    def forward(self, state):\n",
        "        return self.fc2(self.relu(self.fc1(state)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GVAD9La6LKuj"
      },
      "source": [
        "### Implementing REINFORCE with a Baseline (20 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Snk7lrJ4LKuj"
      },
      "source": [
        "To reduce variance in policy updates, we introduce a **baseline** function $( V(s) )$, which estimates the expected return from a given state. Instead of using the full return $( G_t )$, we adjust the policy update by incorporating the baseline:  \n",
        "\n",
        "$$[\n",
        "\\theta \\leftarrow \\theta + \\alpha \\sum_{t=0}^{T} \\nabla_{\\theta} \\log \\pi_{\\theta}(a_t \\mid s_t) (G_t - V(s_t))\n",
        "]$$\n",
        "\n",
        "where:  \n",
        "\n",
        "- $( \\theta )$ are the policy parameters (weights of the neural network).  \n",
        "- $( \\alpha )$ is the learning rate.  \n",
        "- $( G_t )$ is the **discounted return** from timestep $( t )$:  \n",
        "\n",
        "  $$[\n",
        "  G_t = \\sum_{k=0}^{T-t} \\gamma^k R_{t+k}\n",
        "  ]$$\n",
        "\n",
        "- $( V(s_t) )$ is the **baseline function**, estimated by a **value network**, which is trained using mean squared error (MSE) between predicted values and actual returns:  \n",
        "\n",
        "  $$[\n",
        "  L = \\frac{1}{N} \\sum_{t=0}^{N} (G_t - V(s_t))^2\n",
        "  ]$$\n",
        "\n",
        "The introduction of the baseline does not change the expected value of the gradient but significantly **reduces variance**, leading to more stable learning.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "id": "bEqOIcp1LKuj"
      },
      "outputs": [],
      "source": [
        "def reinforce_with_baseline(env, policy_net, baseline_net, policy_optimizer, baseline_optimizer, num_episodes=1000, gamma=0.99):\n",
        "    rewards_per_episode = []\n",
        "    mse = nn.MSELoss()\n",
        "    for episode in range(num_episodes):\n",
        "        state = env.reset()\n",
        "        trajectory, rewards, values = [], [], []\n",
        "\n",
        "        while True:\n",
        "            state_tensor = torch.tensor(state, dtype=torch.float32)\n",
        "            log_probs = policy_net(state_tensor).detach().numpy()\n",
        "            probs = np.exp(log_probs)\n",
        "\n",
        "            value = baseline_net(state_tensor).detach().numpy()\n",
        "\n",
        "            action = np.random.choice(len(probs), p=probs)\n",
        "            new_state, reward, done, _ = env.step(action)\n",
        "\n",
        "            trajectory.append((state, action, reward))\n",
        "            rewards.append(reward)\n",
        "            values.append(value)\n",
        "\n",
        "            state = new_state\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        returns = compute_returns(rewards, gamma)\n",
        "        returns = torch.tensor(returns, dtype=torch.float32)\n",
        "\n",
        "        loss1 = 0\n",
        "        loss2 = 0\n",
        "        for (state, action, _), G in zip(trajectory, returns):\n",
        "            state_tensor = torch.tensor(state, dtype=torch.float32)\n",
        "            log_probs = policy_net(state_tensor)\n",
        "            value = baseline_net(state_tensor)\n",
        "            loss1 -= log_probs[action] * (G - value)\n",
        "            loss2 += (G - value)**2\n",
        "        loss2 /= len(values)\n",
        "\n",
        "        policy_optimizer.zero_grad()\n",
        "        loss1.backward(retain_graph=True)\n",
        "        policy_optimizer.step()\n",
        "\n",
        "        baseline_optimizer.zero_grad()\n",
        "        loss2.backward()\n",
        "        baseline_optimizer.step()\n",
        "\n",
        "\n",
        "        rewards_per_episode.append(sum(rewards))\n",
        "\n",
        "        if (episode + 1) % 50 == 0:\n",
        "            print(f\"Episode {episode + 1}, Reward: {sum(rewards)}\")\n",
        "\n",
        "    return rewards_per_episode"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tILjt3CZ_vby"
      },
      "source": [
        "**Question 3: (5 points)**\n",
        "\n",
        "Why is a baseline introduced in the REINFORCE algorithm, and how does it contribute to training stability?\n",
        "\n",
        ".....</br>\n",
        ".....</br>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yucGANPrLKuj"
      },
      "source": [
        "### Training (10 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tb8u-_PsLKuj"
      },
      "source": [
        "To define key hyperparameters that influence training efficiency and convergence.\n",
        "\n",
        "Hyperparameters like learning rate, discount factor, and number of episodes significantly impact the training process. Proper tuning is necessary to ensure stable learning while avoiding slow convergence or premature convergence to suboptimal policies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQC5FadRpJ3u",
        "outputId": "b3fcd818-1ff8-4d64-ddcc-a9716cd4ae04"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 100, Reward: 63.0\n",
            "Episode 150, Reward: 101.0\n",
            "Episode 200, Reward: 228.0\n",
            "Episode 250, Reward: 333.0\n",
            "Episode 300, Reward: 500.0\n",
            "Episode 350, Reward: 335.0\n",
            "Episode 400, Reward: 127.0\n",
            "Episode 450, Reward: 17.0\n",
            "Episode 500, Reward: 203.0\n",
            "Episode 550, Reward: 318.0\n",
            "Episode 600, Reward: 500.0\n",
            "Episode 650, Reward: 500.0\n",
            "Episode 700, Reward: 166.0\n",
            "Episode 750, Reward: 500.0\n",
            "Episode 800, Reward: 500.0\n",
            "Episode 850, Reward: 104.0\n",
            "Episode 900, Reward: 500.0\n",
            "Episode 950, Reward: 500.0\n",
            "Episode 1000, Reward: 458.0\n",
            "Episode 1050, Reward: 500.0\n",
            "Episode 1100, Reward: 302.0\n",
            "Episode 1150, Reward: 500.0\n",
            "Episode 1200, Reward: 500.0\n",
            "Episode 1250, Reward: 500.0\n",
            "Episode 1300, Reward: 500.0\n",
            "Episode 1350, Reward: 500.0\n",
            "Episode 1400, Reward: 500.0\n",
            "Episode 1450, Reward: 500.0\n",
            "Episode 1500, Reward: 500.0\n",
            "Episode 1550, Reward: 500.0\n",
            "Episode 1600, Reward: 500.0\n",
            "Episode 1650, Reward: 500.0\n",
            "Episode 1700, Reward: 500.0\n",
            "Episode 1750, Reward: 500.0\n",
            "Episode 1800, Reward: 500.0\n",
            "Episode 1850, Reward: 500.0\n",
            "Episode 1900, Reward: 500.0\n",
            "Episode 1950, Reward: 500.0\n",
            "Episode 2000, Reward: 118.0\n",
            "\n",
            "--------------------------------------------------------------------\n",
            "\n",
            "Training REINFORCE with Baseline:\n",
            "Episode 50, Reward: 26.0\n",
            "Episode 100, Reward: 146.0\n",
            "Episode 150, Reward: 87.0\n",
            "Episode 200, Reward: 482.0\n",
            "Episode 250, Reward: 168.0\n",
            "Episode 300, Reward: 176.0\n",
            "Episode 350, Reward: 500.0\n",
            "Episode 400, Reward: 252.0\n",
            "Episode 450, Reward: 500.0\n",
            "Episode 500, Reward: 224.0\n",
            "Episode 550, Reward: 500.0\n",
            "Episode 600, Reward: 500.0\n",
            "Episode 650, Reward: 500.0\n",
            "Episode 700, Reward: 500.0\n",
            "Episode 750, Reward: 500.0\n",
            "Episode 800, Reward: 500.0\n",
            "Episode 850, Reward: 240.0\n",
            "Episode 900, Reward: 500.0\n",
            "Episode 950, Reward: 500.0\n",
            "Episode 1000, Reward: 307.0\n",
            "Episode 1050, Reward: 384.0\n",
            "Episode 1100, Reward: 500.0\n",
            "Episode 1150, Reward: 500.0\n",
            "Episode 1200, Reward: 500.0\n",
            "Episode 1250, Reward: 500.0\n",
            "Episode 1300, Reward: 500.0\n",
            "Episode 1350, Reward: 500.0\n",
            "Episode 1400, Reward: 500.0\n",
            "Episode 1450, Reward: 500.0\n",
            "Episode 1500, Reward: 500.0\n",
            "Episode 1550, Reward: 500.0\n",
            "Episode 1600, Reward: 500.0\n",
            "Episode 1650, Reward: 500.0\n",
            "Episode 1700, Reward: 500.0\n",
            "Episode 1750, Reward: 500.0\n",
            "Episode 1800, Reward: 500.0\n",
            "Episode 1850, Reward: 500.0\n",
            "Episode 1900, Reward: 374.0\n",
            "Episode 1950, Reward: 500.0\n",
            "Episode 2000, Reward: 500.0\n",
            "Training REINFORCE without Baseline:\n",
            "Episode 50, Reward: 89.0\n",
            "Episode 100, Reward: 174.0\n",
            "Episode 150, Reward: 134.0\n",
            "Episode 200, Reward: 241.0\n",
            "Episode 250, Reward: 122.0\n",
            "Episode 300, Reward: 245.0\n",
            "Episode 350, Reward: 446.0\n",
            "Episode 400, Reward: 53.0\n",
            "Episode 450, Reward: 185.0\n",
            "Episode 500, Reward: 106.0\n",
            "Episode 550, Reward: 325.0\n",
            "Episode 600, Reward: 94.0\n",
            "Episode 650, Reward: 500.0\n",
            "Episode 700, Reward: 360.0\n",
            "Episode 750, Reward: 446.0\n",
            "Episode 800, Reward: 307.0\n",
            "Episode 850, Reward: 352.0\n",
            "Episode 900, Reward: 157.0\n",
            "Episode 950, Reward: 136.0\n",
            "Episode 1000, Reward: 141.0\n",
            "Episode 1050, Reward: 114.0\n",
            "Episode 1100, Reward: 240.0\n",
            "Episode 1150, Reward: 500.0\n",
            "Episode 1200, Reward: 500.0\n",
            "Episode 1250, Reward: 500.0\n",
            "Episode 1300, Reward: 500.0\n",
            "Episode 1350, Reward: 500.0\n",
            "Episode 1400, Reward: 500.0\n",
            "Episode 1450, Reward: 500.0\n",
            "Episode 1500, Reward: 500.0\n",
            "Episode 1550, Reward: 500.0\n",
            "Episode 1600, Reward: 500.0\n",
            "Episode 1650, Reward: 233.0\n",
            "Episode 1700, Reward: 158.0\n",
            "Episode 1750, Reward: 198.0\n",
            "Episode 1800, Reward: 269.0\n",
            "Episode 1850, Reward: 291.0\n",
            "Episode 1900, Reward: 300.0\n",
            "Episode 1950, Reward: 253.0\n",
            "Episode 2000, Reward: 262.0\n",
            "\n",
            "--------------------------------------------------------------------\n",
            "\n",
            "Training REINFORCE with Baseline:\n",
            "Episode 50, Reward: 109.0\n",
            "Episode 100, Reward: 47.0\n",
            "Episode 150, Reward: 196.0\n",
            "Episode 200, Reward: 474.0\n",
            "Episode 250, Reward: 296.0\n",
            "Episode 300, Reward: 35.0\n",
            "Episode 350, Reward: 389.0\n",
            "Episode 400, Reward: 500.0\n",
            "Episode 450, Reward: 500.0\n",
            "Episode 500, Reward: 500.0\n",
            "Episode 550, Reward: 500.0\n",
            "Episode 600, Reward: 500.0\n",
            "Episode 650, Reward: 500.0\n",
            "Episode 700, Reward: 500.0\n",
            "Episode 750, Reward: 500.0\n",
            "Episode 800, Reward: 500.0\n",
            "Episode 850, Reward: 500.0\n",
            "Episode 900, Reward: 500.0\n",
            "Episode 950, Reward: 500.0\n",
            "Episode 1000, Reward: 500.0\n",
            "Episode 1050, Reward: 500.0\n",
            "Episode 1100, Reward: 500.0\n",
            "Episode 1150, Reward: 500.0\n",
            "Episode 1200, Reward: 294.0\n",
            "Episode 1250, Reward: 500.0\n",
            "Episode 1300, Reward: 500.0\n",
            "Episode 1350, Reward: 500.0\n",
            "Episode 1400, Reward: 500.0\n",
            "Episode 1450, Reward: 284.0\n",
            "Episode 1500, Reward: 500.0\n",
            "Episode 1550, Reward: 500.0\n",
            "Episode 1600, Reward: 500.0\n",
            "Episode 1650, Reward: 500.0\n",
            "Episode 1700, Reward: 500.0\n",
            "Episode 1750, Reward: 500.0\n"
          ]
        }
      ],
      "source": [
        "INPUT_DIM = env.observation_space.shape[0]\n",
        "OUTPUT_DIM = env.action_space.n\n",
        "LEARNING_RATE = 0.001\n",
        "DISCOUNT_FACTOR = 0.99\n",
        "NUM_EPISODES = 2000\n",
        "\n",
        "# Initialize Policy and Baseline Networks\n",
        "policy_net_no_baseline = PolicyNetwork(INPUT_DIM, OUTPUT_DIM).to(DEVICE)\n",
        "policy_net_with_baseline = PolicyNetwork(INPUT_DIM, OUTPUT_DIM).to(DEVICE)\n",
        "baseline_net = ValueNetwork(INPUT_DIM).to(DEVICE)\n",
        "\n",
        "# Optimizers\n",
        "policy_optimizer_no_baseline = optim.Adam(policy_net_no_baseline.parameters(), lr=LEARNING_RATE)\n",
        "policy_optimizer_with_baseline = optim.Adam(policy_net_with_baseline.parameters(), lr=LEARNING_RATE)\n",
        "baseline_optimizer = optim.Adam(baseline_net.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# Training\n",
        "print(\"Training REINFORCE without Baseline:\")\n",
        "rewards_no_baseline = reinforce(env, policy_net_no_baseline, policy_optimizer_no_baseline, num_episodes=NUM_EPISODES, gamma=DISCOUNT_FACTOR)\n",
        "\n",
        "print(\"\\n--------------------------------------------------------------------\\n\")\n",
        "\n",
        "print(\"Training REINFORCE with Baseline:\")\n",
        "rewards_with_baseline = reinforce_with_baseline(\n",
        "    env, policy_net_with_baseline, baseline_net, policy_optimizer_with_baseline, baseline_optimizer,\n",
        "    num_episodes=NUM_EPISODES, gamma=DISCOUNT_FACTOR\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80uEJPNq_vbz"
      },
      "source": [
        "**Question 4: (5 points)**\n",
        "\n",
        "What are the primary challenges associated with policy gradient methods like REINFORCE?\n",
        "\n",
        ".....</br>\n",
        ".....</br>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-aRu5ZpLKuk"
      },
      "source": [
        "# Plot and Comparing Results (10 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EjGaSiR4pJ3u"
      },
      "outputs": [],
      "source": [
        "# Plotting Results\n",
        "window_size = 50  # Moving average window size\n",
        "\n",
        "# Exponential Moving Average calculation\n",
        "def exponential_moving_average(data, window_size):\n",
        "    alpha = 2 / (window_size + 1)  # Smoothing factor\n",
        "    return np.convolve(data, [alpha * (1 - alpha) ** i for i in range(window_size)], mode='valid')\n",
        "\n",
        "plt.plot(rewards_no_baseline, label=\"Without Baseline\", alpha=0.3, color='tab:blue')\n",
        "plt.plot(rewards_with_baseline, label=\"With Baseline\", alpha=0.3, color='tab:green')\n",
        "\n",
        "# Plotting exponential moving averages\n",
        "plt.plot(exponential_moving_average(rewards_no_baseline, window_size), label=\"Exponential Moving Avg (No Baseline)\", linestyle='--', color='tab:blue')\n",
        "plt.plot(exponential_moving_average(rewards_with_baseline, window_size), label=\"Exponential Moving Avg (With Baseline)\", linestyle='--', color='tab:green')\n",
        "\n",
        "plt.xlabel(\"Episode\")\n",
        "plt.ylabel(\"Total Reward\")\n",
        "plt.legend()\n",
        "plt.title(\"REINFORCE: With vs Without Baseline\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UTTPC9ASpJ3v"
      },
      "outputs": [],
      "source": [
        "mean_reward, std_reward = np.mean(rewards_no_baseline), np.std(rewards_no_baseline)\n",
        "print(f\"No Baseline: mean_reward = {mean_reward:.2f} +/- {std_reward:.4f}\")\n",
        "\n",
        "mean_reward, std_reward = np.mean(rewards_with_baseline), np.std(rewards_with_baseline)\n",
        "print(f\"With Baseline: mean_reward = {mean_reward:.2f} +/- {std_reward:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qV1teZcS_vbz"
      },
      "source": [
        "**Question 5:** (5 points)\n",
        "\n",
        "Based on the results, how does REINFORCE with a baseline compare to REINFORCE without a baseline in terms of performance?\n",
        "\n",
        ".....</br>\n",
        ".....</br>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGbydBZMnAxD"
      },
      "source": [
        "**Question 6:** (5 points)\n",
        "\n",
        "Explain how variance affects policy gradient methods, particularly in the context of estimating gradients from sampled trajectories.\n",
        "\n",
        ".....</br>\n",
        ".....</br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3y-9y6S_LKus"
      },
      "source": [
        "# Simulation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WgaQNFCspJ3v"
      },
      "outputs": [],
      "source": [
        "# Record the simulation using the optimal policy no baseline\n",
        "record_simulation(gym.make(\"CartPole-v1\", render_mode='rgb_array', new_step_api=True), policy_net_no_baseline, \"Video_CartPole_no_baseline\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dzzjPZxMpJ3w"
      },
      "outputs": [],
      "source": [
        "# Record the simulation using the optimal policy with baseline\n",
        "record_simulation(gym.make(\"CartPole-v1\", render_mode='rgb_array', new_step_api=True), policy_net_with_baseline, \"Video_CartPole_with_baseline\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}