\documentclass[12pt]{article}
\usepackage[a4paper, top=2.5cm, bottom=2.5cm, left=1.5cm, right=1.5cm]{geometry}
\usepackage{amsmath, amsfonts, amssymb, mathtools}
\usepackage{fancyhdr, setspace, parskip}
\usepackage{graphicx, caption, subfig, array, multirow}
\usepackage{hyperref, enumitem, cancel}
\usepackage[T1]{fontenc}
\usepackage{tgtermes}
\usepackage[dvipsnames]{xcolor}
\usepackage{tocloft}
\usepackage{titlesec}
\usepackage{lipsum}  

\definecolor{DarkBlue}{RGB}{10, 0, 80}

% Hyperlink setup
\hypersetup{
    colorlinks=true,
    linkcolor=DarkBlue,
    filecolor=BrickRed,      
    urlcolor=RoyalBlue,
}


% Header and footer customization
\fancyhead{}
\fancyhead[L]{
{\fontfamily{lmss}{\color{DarkBlue}
\textbf{\leftmark}
}}
}
\fancyhead[R]{
{\fontfamily{ppl}\selectfont {\color{DarkBlue}
{Deep RL [Spring 2025]}
}}
}

\fancyfoot{}
\fancyfoot[C]{
{\fontfamily{lmss}{\color{BrickRed}
\textbf{\thepage}
}}
}

\renewcommand{\sectionmark}[1]{ \markboth{\thesection\quad #1}{} }

\renewcommand{\headrule}{{\color{BrickRed}\hrule width\headwidth height 0.5pt}}
\renewcommand{\footrulewidth}{0pt}


% Table of Contents customizations
\renewcommand{\cftsecafterpnum}{\vskip6pt}
\renewcommand{\cftsubsecafterpnum}{\vskip3pt}
\renewcommand{\cftsubsubsecafterpnum}{\vskip3pt}
\renewcommand{\cftsecfont}{\sffamily\large}
\renewcommand{\cftsubsecfont}{\sffamily}
\renewcommand{\cftsubsubsecfont}{\sffamily}
% \renewcommand{\cftsecdotsep}{1}
\renewcommand{\cftsubsecdotsep}{1}
\renewcommand{\cftsubsubsecdotsep}{1}


% Section title styles
\titleformat*{\section}{\LARGE\bfseries\color{DarkBlue}}
\titleformat*{\subsection}{\Large\bfseries\color{DarkBlue}}
\titleformat*{\subsubsection}{\large\bfseries\color{DarkBlue}}

\definecolor{light-gray}{gray}{0.95}
\newcommand{\code}[1]{\colorbox{light-gray}{\texttt{#1}}}

% Start of the document
\pagestyle{fancy}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\pagenumbering{gobble}
\thispagestyle{plain}

\begin{center}

\vspace*{-1.5cm}
\begin{figure}[!h]
    \centering
    \includegraphics[width=0.7\linewidth]{figs/cover-std.png}
\end{figure}

{
\fontfamily{ppl}

{\color{DarkBlue} {\fontsize{30}{50} \textbf{
Deep Reinforcement Learning
}}}

{\color{DarkBlue} {\Large
Professor Mohammad Hossein Rohban
}}
}


\vspace{20pt}

{
\fontfamily{lmss}


{\color{RedOrange}
{\Large
Homework 1:
}\\
}
{\color{BrickRed}
\rule{12cm}{0.5pt}

{\Huge
Introduction to RL
}
\rule{12cm}{0.5pt}
}

\vspace{10pt}

{\color{RoyalPurple} { \small By:} } \\
\vspace{10pt}

{\color{Blue} { \LARGE [Parsa Ghezelbash] } } \\
\vspace{5pt}
{\color{RoyalBlue} { \Large [401110437] } }


\vspace*{\fill}
\begin{center}
\begin{tabular}{ccc}
    \includegraphics[width=0.14\linewidth]{figs/sharif-logo.png} & \includegraphics[width=0.14\linewidth]{figs/riml-logo.png} & \includegraphics[width=0.14\linewidth]{figs/dlr-logo.png} \\
\end{tabular}
\end{center}

\vspace*{-.25cm}

{\color{YellowOrange} {
\rule{10cm}{0.5pt} \\
\vspace{2pt}
\large Spring 2025}
}}
\vspace*{-1cm}

\end{center}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage
\pagenumbering{gobble}
\thispagestyle{plain}
{\fontfamily{lmss}\selectfont {\color{BrickRed} \textbf{\tableofcontents} }}

{\fontfamily{lmss}\selectfont {\color{DarkBlue}

\subsection*{Grading}

The grading will be based on the following criteria, with a total of 100 points:

\[
\begin{array}{|l|l|}
\hline
\textbf{Task} & \textbf{Points} \\
\hline
\text{Task 1: Solving Predefined Environments} & 45 \\
\text{Task 2: Creating Custom Environments} & 45 \\

\hline
\text{Clarity and Quality of Code} & 5 \\
\text{Clarity and Quality of Report} & 5 \\
\hline
\text{Bonus 1: Writing a wrapper for a known env} & 10 \\
\text{Bonus 2: Implementing pygame env } & 20 \\
\text{Bonus 3: Writing your report in Latex } & 10 \\
\hline
\end{array}
\]

\textbf{Notes:}
\begin{itemize}
    \item Include well-commented code and relevant plots in your notebook.
    \item Clearly present all comparisons and analyses in your report.
    \item Ensure reproducibility by specifying all dependencies and configurations.
\end{itemize}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage
\pagenumbering{arabic}

{\fontfamily{lmss}\selectfont {\color{DarkBlue}

\section{Task 1: Solving Predefined Environments [45-points]}

\subsection*{Objective}
Train reinforcement learning agents to solve predefined Gymnasium environments (\texttt{CartPole-v1} and \texttt{Taxi-v3}) using PPO and DQN algorithms. Custom reward wrappers were implemented to address sparse rewards and improve learning efficiency.

\subsection*{Approach}
\begin{itemize}
    \item \textbf{Environment Setup}: Used built-in Gymnasium environments with custom reward wrappers.
    \item \textbf{Reward Engineering}:
    \begin{itemize}
        \item Modified rewards to provide dense feedback during training.
        \item Added penalties for undesirable behaviors (e.g., large pole angles in \texttt{CartPole-v1}, long paths in \texttt{Taxi-v3}).
    \end{itemize}
    \item \textbf{Hyperparameter Tuning}: Explored different combinations of learning rates, discount factors, and exploration parameters.
\end{itemize}

\subsection*{Implementation}

\subsubsection*{General Framework}
The following components were implemented for both environments:
\begin{itemize}
    \item \texttt{EpisodeTracker}: Tracks episode rewards and lengths.
    \item \texttt{MetricsCollectorCallback}: Collects training metrics for analysis.
    \item \texttt{train\_hyperparams}: Trains models with different hyperparameter combinations.
    \item \texttt{plot\_hyperparam\_results}: Visualizes training progress.
\end{itemize}

\subsubsection*{Custom Reward Wrappers}

\paragraph*{CartPole-v1}
\begin{verbatim}
class CustomRewardWrapper(RewardWrapper):
    def reward(self, reward):
        obs = self.env.unwrapped.state
        if obs is None:
            obs = self.env.unwrapped._get_obs()
        pole_angle = obs[2]
        return reward - 0.1 * abs(pole_angle)
\end{verbatim}
\textbf{Modifications}:
\begin{itemize}
    \item Added a penalty proportional to the pole angle to discourage large deviations.
\end{itemize}

\paragraph*{Taxi-v3}
\begin{verbatim}
class CustomRewardWrapper(RewardWrapper):
    def reward(self, reward):
        if self.last_obs is not None:
            taxi_row, taxi_col, pass_loc, dest_idx = self.env.unwrapped.decode(self.last_obs)
            distance_penalty = 0.0
            bonus = 0.0
            if pass_loc == 4:
                dest_row, dest_col = self.env.unwrapped.locs[dest_idx]
                distance = abs(taxi_row - dest_row) + abs(taxi_col - dest_col)
                distance_penalty = -0.1 * distance
            if reward == 20:
                bonus = 2.0
            return reward + distance_penalty + bonus
        return reward
\end{verbatim}
\textbf{Modifications}:
\begin{itemize}
    \item Added a penalty for distance to the destination when carrying the passenger.
    \item Amplified the reward for successful dropoffs.
\end{itemize}

\subsection*{Results}

\subsubsection*{CartPole-v1}
\begin{itemize}
    \item \textbf{Standard Rewards}:
    \begin{itemize}
        \item PPO: Achieved a maximum reward of 500 (environment limit).
        \item DQN: Achieved a maximum reward of 415.
    \end{itemize}
    \item \textbf{Modified Rewards}:
    \begin{itemize}
        \item PPO: Achieved a maximum reward of 500 (environment limit).
        \item DQN: Achieved a maximum reward of 498.
    \end{itemize}
\end{itemize}

\subsubsection*{Taxi-v3}
\begin{itemize}
    \item \textbf{Standard Rewards}:
    \begin{itemize}
        \item PPO: Achieved a maximum reward of -12.
        \item DQN: Achieved a maximum reward of -38.
    \end{itemize}
    \item \textbf{Modified Rewards}:
    \begin{itemize}
        \item PPO: Achieved a maximum reward of -10.3.
        \item DQN: Achieved a maximum reward of -69.7.
    \end{itemize}
\end{itemize}

\subsection*{Analysis}
\begin{itemize}
    \item \textbf{Custom Rewards}: Improved learning efficiency by providing intermediate feedback.
    \item \textbf{Hyperparameter Tuning}: Critical for achieving optimal performance.
    \item \textbf{Algorithm Comparison}: PPO outperformed DQN.
\end{itemize}

\subsection*{Visualization}
\begin{itemize}
    \item Training curves for rewards and episode lengths were plotted for each algorithm and reward configuration.
    \item Best-performing configurations were compared to identify optimal hyperparameters.
\end{itemize}

}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage

{\fontfamily{lmss}\selectfont {\color{DarkBlue}

\section{Task 2: Creating Custom Environments [45-points]}

\subsection*{Objective}
Design and train reinforcement learning agents on a custom grid-world environment with obstacles. The goal was to navigate from a start position to a goal while avoiding blocked cells.

\subsection*{Environment Design}
\begin{itemize}
    \item \textbf{State Space}: 2D grid positions (agent and goal) $\rightarrow$ 4D observation.
    \item \textbf{Action Space}: 4 discrete actions (up, down, left, right).
    \item \textbf{Rewards}:
    \begin{itemize}
        \item Step penalty: $-0.1$ per step.
        \item Progress bonus: $+0.5 \times (\text{previous distance} - \text{current distance})$.
        \item Goal reward: $+10$ for reaching the goal.
    \end{itemize}
    \item \textbf{Obstacles}: Fixed blocked positions at $(1, 1)$ and $(2, 2)$.
    \item \textbf{Termination}:
    \begin{itemize}
        \item Reaching the goal.
        \item Exceeding the maximum steps (200).
    \end{itemize}
\end{itemize}

\subsection*{Implementation}

\subsubsection*{Custom Environment Class}
\begin{verbatim}
class YourAwesomeEnvironment(gym.Env):
    def __init__(self) -> None:
        super().__init__()
        self.action_space = spaces.Discrete(4)
        self.observation_space = spaces.Box(
            low=np.array([0, 0]), high=np.array([3, 3]), dtype=int
        )
        self.grid_size = 4
        self.start_pos = (0, 0)
        self.goal_pos = (3, 3)
        self.blocked_positions = [(1, 1), (2, 2)]
        self.state = self.start_pos
        self.max_steps = 200
        self.current_steps = 0
        self.prev_dist = None

    def step(self, action):
        x, y = self.state
        if action == 0: x = max(x - 1, 0)
        elif action == 1: x = min(x + 1, self.grid_size - 1)
        elif action == 2: y = max(y - 1, 0)
        elif action == 3: y = min(y + 1, self.grid_size - 1)

        if (x, y) in self.blocked_positions:
            x, y = self.state

        self.state = (x, y)
        terminated = (x, y) == self.goal_pos
        self.current_steps += 1
        truncated = self.current_steps >= self.max_steps

        goal_dist = abs(x - self.goal_pos[0]) + abs(y - self.goal_pos[1])
        reward = -0.1
        if self.prev_dist is not None:
            progress_bonus = 0.5 * (self.prev_dist - goal_dist)
            reward += progress_bonus
        if terminated:
            reward += 10.0
        self.prev_dist = goal_dist

        return np.array(self.state, dtype=int), reward, terminated, truncated, {}
\end{verbatim}

\subsubsection*{Training Setup}
\begin{itemize}
    \item \textbf{Algorithms}: PPO and DQN.
    \item \textbf{Hyperparameters}:
    \begin{itemize}
        \item PPO: Default parameters with \texttt{learning\_rate=3e-4}.
        \item DQN: Default parameters with \texttt{learning\_rate=1e-3}.
    \end{itemize}
    \item \textbf{Training Duration}: 30,000 timesteps.
    \item \textbf{Evaluation}: 100 episodes with deterministic policy.
\end{itemize}

\subsection*{Results}
\begin{itemize}
    \item \textbf{PPO}:
    \begin{itemize}
        \item Success Rate: \textbf{100\%}.
        \item Average Episode Length: \textbf{6 steps}.
    \end{itemize}
    \item \textbf{DQN}:
    \begin{itemize}
        \item Success Rate: \textbf{0\%}.
        \item Average Episode Length: \textbf{200 steps} (truncation limit).
    \end{itemize}
\end{itemize}

\subsection*{Analysis}
\begin{itemize}
    \item \textbf{PPO Performance}:
    \begin{itemize}
        \item Learned optimal paths around obstacles.
        \item Achieved perfect success rate due to effective reward shaping.
    \end{itemize}
    \item \textbf{DQN Performance}:
    \begin{itemize}
        \item Failed to learn meaningful policies.
        \item Likely due to insufficient exploration or hyperparameter tuning.
    \end{itemize}
    \item \textbf{Reward Shaping}:
    \begin{itemize}
        \item Progress bonus encouraged efficient navigation.
        \item Step penalty prevented infinite loops.
    \end{itemize}
\end{itemize}

\subsection*{Visualization}
\begin{itemize}
    \item \textbf{Training Curves}:
    \begin{itemize}
        \item PPO rewards increased steadily, reaching the maximum.
        \item DQN rewards remained flat, indicating no learning.
    \end{itemize}
    \item \textbf{Agent Paths}:
    \begin{itemize}
        \item PPO consistently reached the goal while avoiding obstacles.
        \item DQN often got stuck or collided with obstacles.
    \end{itemize}
\end{itemize}

}}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage

{\fontfamily{lmss}\selectfont {\color{DarkBlue}

\section{Task 3: Pygame for RL environment [20-points]}


}}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}

