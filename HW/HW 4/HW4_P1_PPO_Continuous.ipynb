{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-11T21:34:29.227718Z",
     "iopub.status.busy": "2025-03-11T21:34:29.227412Z",
     "iopub.status.idle": "2025-03-11T21:34:32.592312Z",
     "shell.execute_reply": "2025-03-11T21:34:32.591176Z",
     "shell.execute_reply.started": "2025-03-11T21:34:29.227696Z"
    },
    "id": "zXysnxJdocoL",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install mujoco==2.3.3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pyCx4Tk_GQ4A"
   },
   "source": [
    "## **Importing Required Libraries**\n",
    "\n",
    "This cell imports all the necessary libraries for implementing **Proximal Policy Optimization (PPO)**. The key components include:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-11T21:34:32.594471Z",
     "iopub.status.busy": "2025-03-11T21:34:32.594102Z",
     "iopub.status.idle": "2025-03-11T21:34:32.599959Z",
     "shell.execute_reply": "2025-03-11T21:34:32.599108Z",
     "shell.execute_reply.started": "2025-03-11T21:34:32.594435Z"
    },
    "id": "XXDB_U6DW7Iy",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions import Categorical\n",
    "from torch.distributions import MultivariateNormal\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch.optim as optim\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import imageio\n",
    "from base64 import b64encode\n",
    "from IPython.display import HTML\n",
    "\n",
    "\n",
    "import logging\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "import numpy as np\n",
    "\n",
    "if is_ipython:\n",
    "    from IPython import display\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kp0xLe9kGQ4C"
   },
   "source": [
    "## **Logging, Visualization, and Video Playback Functions**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-11T21:34:32.602374Z",
     "iopub.status.busy": "2025-03-11T21:34:32.602070Z",
     "iopub.status.idle": "2025-03-11T21:34:32.622842Z",
     "shell.execute_reply": "2025-03-11T21:34:32.622013Z",
     "shell.execute_reply.started": "2025-03-11T21:34:32.602352Z"
    },
    "id": "b0abphr4kMZ1",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "logging.getLogger().setLevel(logging.ERROR)\n",
    "\n",
    "# set up matplotlib\n",
    "plt.ion()\n",
    "plt.xkcd(scale=1, length=100, randomness=2)\n",
    "matplotlib.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "\n",
    "def plot_rewards(sum_of_rewards, show_result=None , bove = None,dn = None):\n",
    "    plt.figure(1)\n",
    "    rewards = torch.tensor(sum_of_rewards, dtype=torch.float)\n",
    "    if show_result is not None:\n",
    "        plt.title(f'{show_result}')\n",
    "    else:\n",
    "        plt.clf()\n",
    "        plt.title('Training...')\n",
    "    if bove is not None:\n",
    "        plt.xlabel(f'{bove}')\n",
    "    else:\n",
    "        plt.xlabel('Episode')\n",
    "    if dn is not None:\n",
    "        plt.xlabel(f'{dn}')\n",
    "    else:\n",
    "        plt.ylabel('Reward')\n",
    "    plt.plot(rewards.numpy())\n",
    "    # Take 50 episode averages and plot them too\n",
    "    length = len(rewards)\n",
    "    init_len = min(49, length)\n",
    "    init_means = np.cumsum(rewards[:init_len]) / (1 + np.arange(init_len))\n",
    "    if length > 50:\n",
    "        means = rewards.unfold(0, 50, 1).mean(1).view(-1)\n",
    "        means = torch.cat((init_means, means))\n",
    "    else:\n",
    "        means = init_means\n",
    "    plt.plot(means.numpy())\n",
    "\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "    if is_ipython:\n",
    "        if not show_result:\n",
    "            display.display(plt.gcf())\n",
    "            display.clear_output(wait=True)\n",
    "        else:\n",
    "            display.display(plt.gcf())\n",
    "\n",
    "def show_video(path):\n",
    "    mp4 = open(path, 'rb').read()\n",
    "    data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
    "    return HTML(\"\"\"\n",
    "    <video width=400 controls>\n",
    "          <source src=\"%s\" type=\"video/mp4\">\n",
    "    </video>\n",
    "    \"\"\" % data_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WnKOvNodGQ4C"
   },
   "source": [
    "## **Memory Buffer for PPO**\n",
    "\n",
    "### **Class: `Memory`**\n",
    "- A **buffer** for storing experiences during training, used for **PPO (Proximal Policy Optimization) updates**.\n",
    "- Stores the following per episode:\n",
    "  - `actions`: Actions taken by the agent.\n",
    "  - `states`: Observed states from the environment.\n",
    "  - `logprobs`: Log probabilities of actions (needed for importance sampling in PPO).\n",
    "  - `rewards`: Rewards obtained after taking actions.\n",
    "  - `state_values`: Estimated state values from the critic.\n",
    "\n",
    "### **Function: `clear()`**\n",
    "- Clears the stored experiences before collecting new trajectories.\n",
    "- Ensures memory is reset between updates to prevent stale data from affecting optimization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-11T21:34:32.624834Z",
     "iopub.status.busy": "2025-03-11T21:34:32.624568Z",
     "iopub.status.idle": "2025-03-11T21:34:32.644027Z",
     "shell.execute_reply": "2025-03-11T21:34:32.643035Z",
     "shell.execute_reply.started": "2025-03-11T21:34:32.624814Z"
    },
    "id": "CZ16f7aiZlul",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Memory:\n",
    "    def __init__(self):\n",
    "        self.actions = []\n",
    "        self.states = []\n",
    "        self.logprobs = []\n",
    "        self.rewards = []\n",
    "        self.state_values = []\n",
    "\n",
    "    def clear(self):\n",
    "        del self.actions[:]\n",
    "        del self.states[:]\n",
    "        del self.logprobs[:]\n",
    "        del self.rewards[:]\n",
    "        del self.state_values[:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x1AK3ZN2GQ4D"
   },
   "source": [
    "## **Actor-Critic Network in PPO**\n",
    "\n",
    "This implementation defines two neural networks used in Proximal Policy Optimization (PPO).\n",
    "\n",
    "The actor network is responsible for predicting a probability distribution over actions (discrete) or estimating the value for each action (continuous), given the current state, while the critic network evaluates how good the action taken by the actor is, by predicting the reward based on state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-11T21:34:32.644934Z",
     "iopub.status.busy": "2025-03-11T21:34:32.644714Z",
     "iopub.status.idle": "2025-03-11T21:34:32.657710Z",
     "shell.execute_reply": "2025-03-11T21:34:32.656868Z",
     "shell.execute_reply.started": "2025-03-11T21:34:32.644908Z"
    },
    "id": "iQdCusXxBfG-",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim, hidden_size, action_dim):\n",
    "        super(Actor, self).__init__()\n",
    "        # Define the Actor architecture\n",
    "        self.linear1 = nn.Linear(state_dim, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.linear3 = nn.Linear(hidden_size, action_dim)\n",
    "\n",
    "        self.log_std = nn.Parameter(torch.zeros(action_dim), requires_grad=True)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.linear1(state))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        mu = F.tanh(self.linear3(x))\n",
    "        std = torch.exp(self.log_std)\n",
    "        \n",
    "        return mu, std\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim, hidden_size):\n",
    "        super(Critic, self).__init__()\n",
    "        \n",
    "        self.linear1 = nn.Linear(state_dim, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.linear3 = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.linear1(state))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        value = self.linear3(x)\n",
    "\n",
    "        return value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u041w-VHGQ4D"
   },
   "source": [
    "## **Proximal Policy Optimization (PPO) Algorithm Implementation**\n",
    "\n",
    "This class implements the **PPO (Proximal Policy Optimization) algorithm**, which is a policy gradient method used in reinforcement learning. It optimizes an actor-critic network while maintaining stability and efficiency.\n",
    "\n",
    "### **Key Components:**\n",
    "1. **Initialization (`__init__`)**:\n",
    "   - Sets hyperparameters (e.g., `gamma`, `eps_clip`, `gae_lambda`).\n",
    "   - Creates **Actor and Critic networks**.\n",
    "   - Defines **Adam optimizers** for both networks.\n",
    "   - Initializes a memory buffer for storing experience.\n",
    "\n",
    "2. **Action Selection (`select_action`)**:\n",
    "   - Uses the **Actor network** to predict a mean and standard deviation for the action.\n",
    "   - Samples an action from a normal distribution.\n",
    "   - Stores the action, log probability, and value function output in memory.\n",
    "\n",
    "3. **Policy Evaluation (`evaluate`)**:\n",
    "   - Computes the log probability of actions under the current policy.\n",
    "   - Returns entropy (used for exploration encouragement).\n",
    "\n",
    "4. **Policy Update (`update`)**:\n",
    "   - Computes **discounted rewards** and **Generalized Advantage Estimation (GAE)**.\n",
    "   - Uses **clipped surrogate objective** to stabilize training.\n",
    "   - Minimizes **actor loss** (policy update) and **critic loss** (value function update).\n",
    "   - Updates networks using **gradient descent**.\n",
    "\n",
    "5. **Memory Handling (`push_memory`, `load_memory`, `store_reward`)**:\n",
    "   - Stores and retrieves experience for training updates.\n",
    "   - Resets the memory buffer after each policy update.\n",
    "\n",
    "### **Key Features of PPO**\n",
    "- **Clipping Ratio**: Prevents excessive updates to the policy, improving stability.\n",
    "- **Advantage Estimation**: Helps in reducing variance in policy updates.\n",
    "- **Entropy Regularization**: Encourages exploration by adding entropy loss.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "weDfdfeMcgCD"
   },
   "source": [
    "## **1. PPO Clipped Loss Function**\n",
    "The PPO loss function prevents large policy updates by introducing a clipped objective. The ratio of new and old policy probabilities is computed as:\n",
    "$$\n",
    "r_t(\\theta) = \\frac{\\pi_{\\theta}(a_t | s_t)}{\\pi_{\\theta_{\\text{old}}}(a_t | s_t)}\n",
    "$$\n",
    "where:\n",
    "- $ \\theta $ are the current policy parameters.\n",
    "- $\\theta_{\\text{old}} $ are the policy parameters before the update.\n",
    "\n",
    "The surrogate loss function is:\n",
    "$$\n",
    "L^{\\text{CLIP}}(\\theta) = \\mathbb{E} \\left[ \\min \\left( r_t(\\theta) A_t, \\text{clip}(r_t(\\theta), 1 - \\epsilon, 1 + \\epsilon) A_t \\right) \\right]\n",
    "$$\n",
    "where:\n",
    "- $ A_t $ is the **advantage function**, which measures how much better an action is compared to the expected value of the state.\n",
    "- $ \\epsilon $ is a small clipping parameter (e.g., **0.2**) that prevents drastic policy updates.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Critic Loss Function**\n",
    "The critic is updated using **Mean Squared Error (MSE)** between the predicted and actual state value:\n",
    "$$\n",
    "L_{\\text{critic}} = \\mathbb{E} \\left[ (V(s_t) - R_t)^2 \\right]\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Total Loss**\n",
    "The overall loss function is:\n",
    "$$\n",
    "L_{\\text{total}} = L^{\\text{CLIP}} + \\beta H - L_{\\text{critic}}\n",
    "$$\n",
    "where:\n",
    "- $ H $ is the **entropy bonus** to encourage exploration.\n",
    "- $ \\beta $ is a small coefficient (e.g., **0.01**) controlling the strength of the entropy bonus.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-11T21:34:32.658825Z",
     "iopub.status.busy": "2025-03-11T21:34:32.658591Z",
     "iopub.status.idle": "2025-03-11T21:34:32.677957Z",
     "shell.execute_reply": "2025-03-11T21:34:32.677098Z",
     "shell.execute_reply.started": "2025-03-11T21:34:32.658800Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class PPO(nn.Module):\n",
    "    def __init__(self, env, config):\n",
    "        super(PPO, self).__init__()\n",
    "        self.gamma = config.GAMMA\n",
    "        self.eps_clip = config.EPS_CLIP\n",
    "        self.epochs = config.K_EPOCHS\n",
    "        self.gae_lambda = config.GAE\n",
    "        self.hidden_size = config.hidden_size\n",
    "\n",
    "        self.lr = config.LEARNING_RATE\n",
    "        state_dim = env.observation_space.shape[0]\n",
    "        action_dim = env.action_space.shape[0]\n",
    "\n",
    "        self.actor = Actor(state_dim, self.hidden_size, action_dim).to(device)\n",
    "        self.critic = Critic(state_dim, self.hidden_size).to(device)\n",
    "\n",
    "        self.optimizer_actor = optim.Adam(self.actor.parameters(), lr=self.lr)\n",
    "        self.optimizer_critic = optim.Adam(self.critic.parameters(), lr=self.lr)\n",
    "        self.memory = Memory()\n",
    "\n",
    "        self.env = env\n",
    "\n",
    "    def select_action(self, state):\n",
    "        # Save state, action, log probability and state value of current step in the memory buffer.\n",
    "        # predict the actions by sampling from a normal distribution\n",
    "        # based on the mean and std calculated by actor\n",
    "        with torch.no_grad():\n",
    "            mu, std = self.actor(state)\n",
    "            cov = torch.diag(std**2)\n",
    "            dist = MultivariateNormal(mu, covariance_matrix=cov)\n",
    "            action = dist.sample()\n",
    "            state_value = self.critic(state)\n",
    "        self.push_memory(state, action, dist.log_prob(action), state_value)\n",
    "\n",
    "        return action.squeeze(0).cpu().numpy()\n",
    "\n",
    "\n",
    "    def evaluate(self, state, action):\n",
    "        # evaluate the state value of this state and log probability of choosing this action\n",
    "        mu, std = self.actor(state)\n",
    "        cov = torch.diag(std**2)\n",
    "        dist = MultivariateNormal(mu, covariance_matrix=cov)\n",
    "        action_logprobs = dist.log_prob(action)\n",
    "        state_value = self.critic(state).squeeze()\n",
    "        entropy = dist.entropy()\n",
    "        \n",
    "        return action_logprobs, state_value, entropy\n",
    "\n",
    "    def update(self):\n",
    "        rewards = torch.tensor(self.memory.rewards, dtype=torch.float32, device=device)\n",
    "        advantages = []\n",
    "        discounted_reward = 0\n",
    "        gae = 0\n",
    "\n",
    "        # load saved states, actions, log probs, and state values\n",
    "        states, actions, log_probs, state_values = self.load_memory()\n",
    "        state_values = state_values.squeeze()\n",
    "\n",
    "        # Calculate gae for each timestep\n",
    "        for t in reversed(range(len(rewards))):\n",
    "            if t == len(rewards) - 1:\n",
    "                dt = rewards[t] - state_values[t]\n",
    "            else:\n",
    "                dt = rewards[t] + self.gamma * state_values[t+1] - state_values[t]\n",
    "            gae = self.gae_lambda * self.gamma * gae + dt\n",
    "            advantages.insert(0, gae)\n",
    "        \n",
    "\n",
    "        advantages = torch.tensor(advantages, dtype=torch.float32, device=device)\n",
    "        # advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "\n",
    "        returns = advantages + state_values\n",
    "\n",
    "        loss_ac = 0\n",
    "        loss_cri = 0\n",
    "        for _ in range(self.epochs):\n",
    "            # calculate logprobs and state values based on the new policy\n",
    "            action_logprobs, state_values, entropy = self.evaluate(states, actions)\n",
    "            probs_ratio = torch.exp(action_logprobs - log_probs)\n",
    "            \n",
    "            # Calculate the loss function and perform the optimization\n",
    "            s1 = probs_ratio * advantages\n",
    "            s2 = torch.clamp(probs_ratio, 1 - self.eps_clip, 1 + self.eps_clip) * advantages\n",
    "            loss_actor = -torch.min(s1, s2) + 0.01 * entropy\n",
    "            # print(loss_actor.shape)\n",
    "            \n",
    "            loss_critic = (state_values - returns)**2\n",
    "            # print(state_values.shape, returns.shape)\n",
    "            # print(loss_critic.shape)\n",
    "\n",
    "            loss_actor = loss_actor.mean()\n",
    "            loss_critic = loss_critic.mean()\n",
    "\n",
    "\n",
    "            self.optimizer_actor.zero_grad()\n",
    "            loss_actor.backward(retain_graph=True)\n",
    "            loss_ac += loss_actor.item()\n",
    "            self.optimizer_actor.step()\n",
    "\n",
    "            self.optimizer_critic.zero_grad()\n",
    "            loss_critic.backward()\n",
    "            loss_cri += loss_critic.item()\n",
    "            self.optimizer_critic.step()\n",
    "        # clear the buffer\n",
    "        self.memory.clear()\n",
    "        return loss_ac, loss_cri\n",
    "\n",
    "    def push_memory(self, state, action, log_prob, value):\n",
    "        self.memory.states.append(state)\n",
    "        self.memory.actions.append(action)\n",
    "        self.memory.logprobs.append(log_prob)\n",
    "        self.memory.state_values.append(value)\n",
    "\n",
    "    def load_memory(self):\n",
    "        old_states = torch.stack(self.memory.states).detach()\n",
    "        old_actions = torch.stack(self.memory.actions).detach()\n",
    "        old_logprobs = torch.stack(self.memory.logprobs).detach()\n",
    "        old_state_vals = torch.stack(self.memory.state_values).detach()\n",
    "\n",
    "        return old_states, old_actions, old_logprobs, old_state_vals\n",
    "\n",
    "    def store_reward(self, reward):\n",
    "      self.memory.rewards.append(reward)\n",
    "\n",
    "    def load_reward(self):\n",
    "        return torch.stack(self.memory.rewards).detach()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iq8ZxyStGQ4D"
   },
   "source": [
    "## **Configuration Settings for PPO Training**\n",
    "\n",
    "The `Config` class defines **hyperparameters and settings** for training the PPO agent in a reinforcement learning environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-11T21:34:32.678954Z",
     "iopub.status.busy": "2025-03-11T21:34:32.678730Z",
     "iopub.status.idle": "2025-03-11T21:34:32.688789Z",
     "shell.execute_reply": "2025-03-11T21:34:32.688068Z",
     "shell.execute_reply.started": "2025-03-11T21:34:32.678932Z"
    },
    "id": "6KRaYeHb--KF",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Config:\n",
    "    # Environment settings\n",
    "    SEED = 111\n",
    "    MAX_EPISODES = 3000\n",
    "    \n",
    "\n",
    "    # PPO Hyperparameters\n",
    "    K_EPOCHS = 5\n",
    "    EPS_CLIP = 0.2\n",
    "    GAMMA = 0.99\n",
    "    LEARNING_RATE = 0.001\n",
    "    BETAS = (0.9, 0.99)\n",
    "\n",
    "    hidden_size= 64\n",
    "    GAE = 0.95\n",
    "\n",
    "    DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iRL9eIBWGQ4D"
   },
   "source": [
    "# **Proximal Policy Optimization (PPO) Training on HalfCheetah-v4 Environment**\n",
    "\n",
    "This notebook implements and trains a **Proximal Policy Optimization (PPO)** agent on the **HalfCheetah-v4** environment. The PPO algorithm is an on-policy reinforcement learning method that uses a clipped objective function to update the policy in a stable way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 979
    },
    "execution": {
     "iopub.execute_input": "2025-03-11T21:36:38.232494Z",
     "iopub.status.busy": "2025-03-11T21:36:38.232122Z",
     "iopub.status.idle": "2025-03-11T21:39:58.085832Z",
     "shell.execute_reply": "2025-03-11T21:39:58.085160Z",
     "shell.execute_reply.started": "2025-03-11T21:36:38.232466Z"
    },
    "id": "-otjxE9T3TED",
    "outputId": "5bfbcad7-0f43-4fca-c48c-73ed971303b6",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"HalfCheetah-v4\")\n",
    "config = Config()\n",
    "\n",
    "agent  = PPO(env, config)\n",
    "\n",
    "# We need to train for many more steps to achieve acceptable results compared to the last environment\n",
    "actor_losses = []\n",
    "critic_losses = []\n",
    "rewards_history = []\n",
    "sum_of_rewards = []\n",
    "for episode in range(config.MAX_EPISODES):\n",
    "    state, _ = env.reset()\n",
    "    episode_reward = 0\n",
    "    # write the training loop\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        state = torch.tensor(state, dtype=torch.float32).to(device)\n",
    "        action = agent.select_action(state)\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        agent.store_reward(reward)\n",
    "        episode_reward += reward\n",
    "        state = next_state\n",
    "        done = terminated or truncated\n",
    "        \n",
    "    \n",
    "    loss_ac, loss_cri = agent.update()\n",
    "    actor_losses.append(loss_ac)\n",
    "    critic_losses.append(loss_cri)\n",
    "    rewards_history.append(episode_reward)\n",
    "\n",
    "    if episode % 100 == 0:\n",
    "        print(f'Episode:{episode}->\\t Reward:{rewards_history[-1]}->\\t actor loss:{loss_ac:.6f},\\t critic loss:{loss_cri:.6f}')\n",
    "\n",
    "env.close()\n",
    "\n",
    "plot_rewards(rewards_history, show_result='PPO agent in HalfCheetah env')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wjZfm8xUGQ4E"
   },
   "source": [
    "# **Evaluating the PPO Agent in the HalfCheetah-v4 Environment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 511
    },
    "execution": {
     "iopub.execute_input": "2025-03-11T21:40:34.677062Z",
     "iopub.status.busy": "2025-03-11T21:40:34.676759Z",
     "iopub.status.idle": "2025-03-11T21:41:16.056275Z",
     "shell.execute_reply": "2025-03-11T21:41:16.054698Z",
     "shell.execute_reply.started": "2025-03-11T21:40:34.677038Z"
    },
    "id": "-Bxmb6Z3ooxa",
    "outputId": "8738d0c7-bc57-4b39-ca72-f809a3704c0c",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Create environment\n",
    "%env MUJOCO_GL=egl\n",
    "env = gym.make(\"HalfCheetah-v4\", render_mode=\"rgb_array\")\n",
    "frames = []\n",
    "\n",
    "state, info = env.reset()\n",
    "done = False\n",
    "total_reward = 0\n",
    "\n",
    "# run the learned PPO agent to evaluate it\n",
    "while not done:\n",
    "    state = torch.tensor(state, dtype=torch.float32).to(device)\n",
    "    action = agent.select_action(state)\n",
    "    next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "    \n",
    "    frame = env.render()\n",
    "    frames.append(frame)\n",
    "    \n",
    "    state = next_state\n",
    "    total_reward += reward\n",
    "    done = terminated or truncated\n",
    "\n",
    "\n",
    "\n",
    "env.close()\n",
    "print(f\"Total Reward: {total_reward}\")\n",
    "\n",
    "# Save frames as a video\n",
    "video_path = \"./agent_evaluation.mp4\"\n",
    "imageio.mimsave(video_path, frames, fps=25)\n",
    "\n",
    "# Display the video\n",
    "show_video(video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n586eE68_N37",
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "libreface_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
